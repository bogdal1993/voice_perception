import copyimport randomimport osimport timeimport sysfrom llama_cpp import LlamaSYSTEM_PROMPT = ""SYSTEM_TOKEN = 1587USER_TOKEN = 2188BOT_TOKEN = 12435LINEBREAK_TOKEN = 13ROLE_TOKENS = {	"user": USER_TOKEN,	"bot": BOT_TOKEN,	"system": SYSTEM_TOKEN}def get_message_tokens(model, role, content):	message_tokens = model.tokenize(content.encode("utf-8"))	#message_tokens.insert(1, ROLE_TOKENS[role])	message_tokens.insert(2, LINEBREAK_TOKEN)	message_tokens.append(model.token_eos())	return message_tokensdef get_system_tokens(model):	system_message = {"role": "", "content": SYSTEM_PROMPT}	return get_message_tokens(model, **system_message)#model_name = "mistral7b_dialsum_Q4_K_M.gguf"model_name = "mistral7b_dialsum_Q6_K.gguf"model = Llama(	model_path=model_name,	n_ctx=2000,	n_parts=1,	n_gpu_layers=45)max_new_tokens = 256def bot(history, top_p=0.9, top_k=40, temp=0.01):	tokens = get_system_tokens(model)[:]	tokens.append(LINEBREAK_TOKEN)	message_tokens = get_message_tokens(model=model, role="", content=history)	tokens.extend(message_tokens)	role_tokens = [model.token_bos(), LINEBREAK_TOKEN]	tokens.extend(role_tokens)	print(len(tokens))	generator = model.generate(		tokens,		top_k=top_k,		top_p=top_p,		temp=temp	)	partial_text = ""	for i, token in enumerate(generator):		if token == model.token_eos() or (max_new_tokens is not None and i >= max_new_tokens):			break		partial_text += model.detokenize([token]).decode("utf-8", "ignore")		if partial_text.endswith("Суммаризация диалога"):			partial_text = partial_text[:-20]			break	return partial_text				from fastapi import FastAPIfrom fastapi.middleware.cors import CORSMiddlewarefrom pydantic import BaseModelapp = FastAPI()app.add_middleware(	CORSMiddleware,	allow_origins=["*"],	allow_credentials=True,	allow_methods=["*"],	allow_headers=["*"],)class Speech(BaseModel):	text: str@app.post("/llm/predictsum")async def predict(speech: Speech):	res = bot(speech.text)	return {"message": res}